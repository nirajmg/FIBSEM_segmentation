{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "tropical-likelihood",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "error",
     "timestamp": 1643325445776,
     "user": {
      "displayName": "Niraj Gupta",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "13372498150642930796"
     },
     "user_tz": 420
    },
    "id": "tropical-likelihood",
    "outputId": "15b7b270-1fd4-4fc6-e8f7-675e3362e731"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import napari\n",
    "import numpy as np\n",
    "from tifffile import imwrite\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "\n",
    "from tifffile import imread, imwrite\n",
    "from skimage import exposure\n",
    "from pyMSDtorch.core.networks import MSDNet, TUNet\n",
    "from pyMSDtorch.core import helpers, train_scripts, custom_optimizers, custom_losses, corcoef\n",
    "\n",
    "import glob\n",
    "import qlty\n",
    "import einops\n",
    "from PIL import Image\n",
    "from qlty import qlty2D\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sporting-dictionary",
   "metadata": {
    "id": "sporting-dictionary"
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hollywood-generation",
   "metadata": {
    "id": "hollywood-generation"
   },
   "outputs": [],
   "source": [
    "def display(array1, array2):\n",
    "    \"\"\"\n",
    "    Displays ten random images from each one of the supplied arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    n = 7\n",
    "\n",
    "    indices = np.random.randint(len(array1), size=n)\n",
    "    print('The indices of the images are ', indices)\n",
    "    images1 = array1[indices, :]\n",
    "    images2 = array2[indices, :]\n",
    "\n",
    "    plt.figure(figsize=(50, 20))\n",
    "    for i, (image1, image2) in enumerate(zip(images1, images2)):\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        plt.imshow(image1, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "        ax = plt.subplot(2, n, i + 1 + n)\n",
    "        plt.imshow(image2, vmin=0, vmax=1)\n",
    "        plt.gray()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def regression_metrics( preds, target):\n",
    "    tmp = corcoef.cc(preds.cpu().flatten(), target.cpu().flatten() )\n",
    "    return(tmp)\n",
    "\n",
    "\n",
    "def segment_imgs(testloader, net):\n",
    "    \"\"\" Modified for input and no ground truth\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    seg_imgs = []\n",
    "    noisy_imgs = []\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(testloader):\n",
    "            noisy = batch\n",
    "            noisy = noisy[0]\n",
    "            noisy = torch.FloatTensor(noisy)\n",
    "            noisy = noisy.to(device)#.unsqueeze(1)\n",
    "            \n",
    "            output = net(noisy)\n",
    "\n",
    "            # Compute Pearson Correlation\n",
    "            #tmp =  regression_metrics(output, target)\n",
    "            #running_CC_test_val += tmp.item()\n",
    "\n",
    "            if counter == 0:\n",
    "                seg_imgs = output.detach().cpu()\n",
    "                noisy_imgs = noisy.detach().cpu()\n",
    "            else:\n",
    "                seg_imgs = torch.cat((seg_imgs, output.detach().cpu()), 0)\n",
    "                noisy_imgs = torch.cat((noisy_imgs, noisy.detach().cpu()), 0)\n",
    "            counter+=1\n",
    "            del output\n",
    "            del noisy\n",
    "            torch.cuda.empty_cache()\n",
    "    return seg_imgs, noisy_imgs\n",
    "\n",
    "def create_network(model_type, params):\n",
    "    # set model parameters and initialize the network\n",
    "    if model_type == \"SMSNet\":\n",
    "        net = SMSNet.random_SMS_network(**params)\n",
    "        model_params = {\n",
    "          \"in_channels\": net.in_channels,\n",
    "          \"out_channels\": net.out_channels,\n",
    "          \"in_shape\": net.in_shape,\n",
    "          \"out_shape\": net.out_shape,\n",
    "          \"scaling_table\": net.scaling_table,\n",
    "          \"network_graph\": net.network_graph,\n",
    "          \"channel_count\": net.channel_count,\n",
    "          \"convolution_kernel_size\": net.convolution_kernel_size,\n",
    "          \"first_action\": net.first_action,\n",
    "          \"hidden_action\": net.hidden_action,\n",
    "          \"last_action\":net.last_action,\n",
    "        }\n",
    "        return net, model_params\n",
    "    elif model_type == \"MSDNet\":\n",
    "        net = MSDNet.MixedScaleDenseNetwork(**params)\n",
    "        return net, params\n",
    "    elif model_type == \"MSDNet_partialconv\":\n",
    "        net = MSDNet.MixedScaleDenseNetwork(**params)\n",
    "        return net, params\n",
    "    elif model_type == \"UNet\":\n",
    "        net = UNet.UNet(**params)\n",
    "        return net, params\n",
    "    elif model_type == 'TUNet':\n",
    "        net = TUNet.TUNet(**params)\n",
    "        return net, params\n",
    "    elif model_type == 'TUNet_Eric':\n",
    "        net = TUNet_Eric.TUNet(**params)\n",
    "        return net, params\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-commons",
   "metadata": {
    "id": "ranging-commons"
   },
   "source": [
    "### Load data \n",
    "\n",
    "Data has been saved using the parse_and_save notebook, which simply recasts data as numpy arrays/tiff files.\n",
    "\n",
    "Data was saved in the original resolution, and downsampled by factors of 2 and 4. We load images of the original resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-causing",
   "metadata": {
    "id": "expired-causing"
   },
   "source": [
    "### Prep data for network ingestion\n",
    "\n",
    "We cast data as a tensor and load into pytorch Dataloader framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a779fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/data/FIBSEM/nuclie_align_ROI1_membrane_1700/nuclie01'\n",
    "files = len([f for f in os.listdir(folder) if f.endswith('.jpg')])\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dd70a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = []\n",
    "\n",
    "for file in range(0,files):\n",
    "    img = Image.open(f'{folder}/{file:05}.jpg')\n",
    "    img.load()\n",
    "    img = np.array(img, dtype='float32')\n",
    "    test_imgs.append(img)\n",
    "\n",
    "test_imgs = np.array(test_imgs)\n",
    "test_imgs = np.expand_dims(test_imgs, axis=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f006ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "finite-winner",
   "metadata": {
    "id": "finite-winner"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0    #increase to 1 or 2 with multiple GPUs\n",
    "\n",
    "test_data = TensorDataset(torch.Tensor(test_imgs))\n",
    "\n",
    "test_loader_params = {'batch_size': batch_size,\n",
    "                     'shuffle': False,\n",
    "                     'num_workers': num_workers,\n",
    "                     'pin_memory':True,\n",
    "                     'drop_last': False}\n",
    "\n",
    "test_loader = DataLoader(test_data, **test_loader_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hydraulic-garage",
   "metadata": {
    "id": "hydraulic-garage"
   },
   "source": [
    "### Instantiate network and load trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-flooring",
   "metadata": {
    "id": "express-flooring",
    "outputId": "5775e2af-b5d9-4e61-a63e-a201ead1798f"
   },
   "outputs": [],
   "source": [
    "maindir = '/data/FIBSEM/napari_venv/training_scripts/networks_output/tunet3_1700/tunet3'\n",
    "patterndir = '/data/FIBSEM/napari_venv/training_scripts/networks_output/tunet3_1700_patterns/'\n",
    "params = np.load(maindir + '/params.npy', allow_pickle=True)\n",
    "params_patterns = np.load(patterndir + '/params.npy', allow_pickle=True)\n",
    "\n",
    "#params = params[()]  # Weird trick for loading dictionaries saves as arrays\n",
    "params,params_patterns = params[0],params_patterns[0]\n",
    "print(type(params))\n",
    "print('The following define the network parameters: ', params)\n",
    "print('The following define the network parameters for patterns: ',params_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e00321e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mem= '/data/FIBSEM/napari_venv/training_scripts/networks_output/tunet3_membrane/'\n",
    "params_mem = np.load(mem + '/params.npy', allow_pickle=True)\n",
    "params_mem=params_mem[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-calculation",
   "metadata": {
    "id": "spiritual-calculation",
    "outputId": "c3362265-7a6e-40ad-c3ef-81d43e78c5ca"
   },
   "outputs": [],
   "source": [
    "model_type = 'TUNet'\n",
    "#model_type = 'MSDNet'   # Use only if loading msdnet folder \n",
    "\n",
    "# Initialize correct network architecture\n",
    "net, model_params = create_network(model_type, params)\n",
    "net_patterns, model_params_patterns = create_network(model_type, params_patterns)\n",
    "net_mem, model_params_mem = create_network(model_type, params_mem)\n",
    "\n",
    "# Load trained network parameters\n",
    "net.load_state_dict(torch.load(maindir + '/net_best'))\n",
    "net_mem.load_state_dict(torch.load(mem + '/net_best'))\n",
    "\n",
    "net_patterns.load_state_dict(torch.load(patterndir + '/net_best'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-attempt",
   "metadata": {
    "id": "prescribed-attempt"
   },
   "source": [
    "### Segment and save\n",
    "\n",
    "Device is either cpu or cuda:0 (for a graphics card). We also get summary of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contained-fraud",
   "metadata": {
    "id": "contained-fraud",
    "outputId": "4ca52b63-6089-42d8-e861-3471e5a13349",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = helpers.get_device()\n",
    "device='cuda:1'\n",
    "print('Device we compute on: ', device)\n",
    "\n",
    "net.to(device)\n",
    "net_patterns.to(device)\n",
    "net_mem.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa72ca41",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0    #increase to 1 or 2 with multiple GPUs\n",
    "nucleoluspath = folder+'/nucleolus/'\n",
    "chromopath = folder+'/chromosome/'\n",
    "\n",
    "\n",
    "for i in range(0,files,100):\n",
    "    test_data = TensorDataset(torch.Tensor(test_imgs[i:i+100]))\n",
    "    if os.path.isdir(nucleoluspath) is False:\n",
    "        os.mkdir(nucleoluspath)\n",
    "        os.mkdir(chromopath)\n",
    "        \n",
    "    test_loader_params = {'batch_size': batch_size,\n",
    "                         'shuffle': False,\n",
    "                         'num_workers': num_workers,\n",
    "                         'pin_memory':True,\n",
    "                         'drop_last': False}\n",
    "\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)\n",
    "    output, input_imgs  = segment_imgs(test_loader, net)\n",
    "    output = torch.squeeze(output, 1)\n",
    "    input_imgs = torch.squeeze(input_imgs, 1)\n",
    "    tunet3_output = torch.argmax(output.cpu()[:,:,:,:].data, dim=1)\n",
    "    imgs,masks = input_imgs.numpy(),tunet3_output.numpy()\n",
    "    idx=(masks==2)\n",
    "    chromosome=np.zeros(imgs.shape)\n",
    "    chromosome[idx]=imgs[idx]\n",
    "\n",
    "    idx=(masks==3)\n",
    "    nucleolus=np.zeros(imgs.shape)\n",
    "    nucleolus[idx]=imgs[idx]\n",
    "    \n",
    "    del output\n",
    "    del tunet3_output\n",
    "    del input_imgs\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    \n",
    "    for j in range(nucleolus.shape[0]):\n",
    "        name = f'{i+j:03}.jpg'\n",
    "        Image.fromarray(nucleolus[j].astype(np.uint8)).save(nucleoluspath+name)\n",
    "        Image.fromarray(chromosome[j].astype(np.uint8)).save(chromopath+name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1852c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0    #increase to 1 or 2 with multiple GPUs\n",
    "\n",
    "patternpath = folder+'/pattern/'  \n",
    "\n",
    "\n",
    "for i in range(0,files,100):\n",
    "    test_data = TensorDataset(torch.Tensor(test_imgs[i:i+100]))\n",
    "    if os.path.isdir(patternpath) is False:\n",
    "        \n",
    "        os.mkdir(patternpath)\n",
    "        \n",
    "    test_loader_params = {'batch_size': batch_size,\n",
    "                         'shuffle': False,\n",
    "                         'num_workers': num_workers,\n",
    "                         'pin_memory':True,\n",
    "                         'drop_last': False}\n",
    "\n",
    "\n",
    "\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)\n",
    "    output_patterns, input_imgs  = segment_imgs(test_loader, net_patterns)\n",
    "    output_patterns = torch.squeeze(output_patterns, 1)\n",
    "    input_imgs = torch.squeeze(input_imgs, 1)\n",
    "    tunet3_output_patterns = torch.argmax(output_patterns.cpu()[:,:,:,:].data, dim=1)\n",
    "    imgs,patterns = input_imgs.numpy(),tunet3_output_patterns.numpy()\n",
    "\n",
    "    idx=(patterns==4)\n",
    "    pattern=np.zeros(patterns.shape)\n",
    "    pattern[idx]=imgs[idx]\n",
    "\n",
    "    for j in range(pattern.shape[0]):\n",
    "        name = f'{i+j:03}.jpg'\n",
    "        Image.fromarray(pattern[j].astype(np.uint8)).save(patternpath+name)\n",
    "\n",
    "    del output_patterns\n",
    "    del input_imgs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985f3a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_workers = 0    #increase to 1 or 2 with multiple GPUs\n",
    "\n",
    "mempath = folder+'/membrane/'  \n",
    "\n",
    "\n",
    "for i in range(0,files,100):\n",
    "    test_data = TensorDataset(torch.Tensor(test_imgs[i:i+100]))\n",
    "    if os.path.isdir(mempath) is False:\n",
    "        os.mkdir(mempath)\n",
    "        \n",
    "    test_loader_params = {'batch_size': batch_size,\n",
    "                         'shuffle': False,\n",
    "                         'num_workers': num_workers,\n",
    "                         'pin_memory':True,\n",
    "                         'drop_last': False}\n",
    "\n",
    "\n",
    "    test_loader = DataLoader(test_data, **test_loader_params)\n",
    "    output, input_imgs  = segment_imgs(test_loader, net_mem)\n",
    "    output = torch.squeeze(output, 1)\n",
    "    input_imgs = torch.squeeze(input_imgs, 1)\n",
    "    tunet3_output = torch.argmax(output.cpu()[:,:,:,:].data, dim=1)\n",
    "    imgs,mem = input_imgs.numpy(),tunet3_output.numpy()\n",
    "\n",
    "    idx=(mem==2)\n",
    "    mem=np.zeros(mem.shape)\n",
    "    mem[idx]=imgs[idx]\n",
    "\n",
    "    for j in range(mem.shape[0]):\n",
    "        name = f'{i+j:03}.jpg'\n",
    "        Image.fromarray(mem[j].astype(np.uint8)).save(mempath+name)\n",
    "\n",
    "    del output\n",
    "    del input_imgs\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5e88e6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306\n"
     ]
    }
   ],
   "source": [
    "folder = '/data/FIBSEM/nuclie_align_ROI2_1700/nuclie02/full_nuclie/chromosome'\n",
    "files = len([f for f in os.listdir(folder) if f.endswith('.jpg')])\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455c3b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_imgs = []\n",
    "import glob\n",
    "from os import walk\n",
    "files = []\n",
    "for file in glob.glob(f'{folder}/*.jpg'):\n",
    "    files.append(file)\n",
    "def last_4chars(x):\n",
    "    return int(os.path.basename(x)[:-4])\n",
    "\n",
    "files = sorted(files,key = last_4chars) \n",
    "\n",
    "for file in files:\n",
    "    img = Image.open(file)\n",
    "    img.load()\n",
    "    img = np.array(img, dtype='float32')\n",
    "    idx=(img!=0)\n",
    "    chr=np.zeros(img.shape)\n",
    "    chr[idx]=2\n",
    "    test_imgs.append(chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cc3d \n",
    "labels_out, N = cc3d.largest_k(test_imgs, k=6, connectivity=26, delta=10,return_N=True)\n",
    "viewer = napari.view_image(labels_out, name='nucl1')\n",
    "imwrite(f'{folder}/chromosomes_split_12.tif', labels_out)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LoadModels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
